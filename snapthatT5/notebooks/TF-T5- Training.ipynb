{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href= \"https://www.snapthat.ai/\"> <img src=\"https://www.snapthat.ai/img/landing/images/logo_h1.png\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training T5 using Native Tensorflow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate training using tensorflow 2 and keras. This notebook includes tf Data pipelines for build any other NLP task in a text to text fashion. Anyone can adapt the data pipeline to thier own datasets for Text-2-Text fashion training.\n",
    "#### Features\n",
    "- Train TF T5 on SQUAD questioning and answering\n",
    "- Train T5 using keras trainer fucntion\n",
    "- tf.Data pipeline\n",
    "- TF datasets as source\n",
    "- Log metrics using tensorboard\n",
    "- Profile your experiment with the brand new tensorflow profiler !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aWoa3AtDXN0"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (TFAutoModelWithLMHead, AutoTokenizer, \n",
    "    TFTrainer, TFTrainingArguments, TFT5ForConditionalGeneration, T5Config)\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HW4rT8FODXN3"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_THREAD_MODE\"]=\"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQ5CoTk-DXN6",
    "outputId": "e10031cf-7e75-4012-e4a4-7ac3ecee9d1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRMGSskHDXOA"
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/ml/workspace/ai/TF-T5-text-to-text/data\"\n",
    "\n",
    "log_dir = f\"{data_dir}/experiments/t5/logs\"\n",
    "save_path = f\"{data_dir}/experiments/t5/models\"\n",
    "cache_path_train = f\"{data_dir}/cache/t5.train\"\n",
    "cache_path_test = f\"{data_dir}/cache/t5.test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xcGqd9qDXOF"
   },
   "outputs": [],
   "source": [
    "class SnapthatT5(TFT5ForConditionalGeneration):\n",
    "    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_tracker= tf.keras.metrics.Mean(name='loss') \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y  = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(x, training=True)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        lr = self.optimizer._decayed_lr(tf.float32)\n",
    "#         lr = lr if tf.is_tensor(optimizer.lr) else optimzer.lr(optimizer.)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        metrics = {m.name: m.result() for m in self.metrics}\n",
    "        metrics.update({'lr': lr})\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        output = self(x, training=False)\n",
    "        loss = output[0]\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        logits = output[1]\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvOrIgzODXOD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EgTphQODXOH"
   },
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KX1rsWgZDXOK",
    "outputId": "8967c22c-732a-422f-9e1d-8462d23ecb0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: squad/plain_text\n",
      "INFO:absl:Load dataset info from /home/ml/tensorflow_datasets/squad/plain_text/1.0.0\n",
      "INFO:absl:Reusing dataset squad (/home/ml/tensorflow_datasets/squad/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from /home/ml/tensorflow_datasets/squad/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: squad/plain_text\n",
      "INFO:absl:Load dataset info from /home/ml/tensorflow_datasets/squad/plain_text/1.0.0\n",
      "INFO:absl:Reusing dataset squad (/home/ml/tensorflow_datasets/squad/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from /home/ml/tensorflow_datasets/squad/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='squad',\n",
      "    version=1.0.0,\n",
      "    description='Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.',\n",
      "    homepage='https://rajpurkar.github.io/SQuAD-explorer/',\n",
      "    features=FeaturesDict({\n",
      "        'answers': Sequence({\n",
      "            'answer_start': tf.int32,\n",
      "            'text': Text(shape=(), dtype=tf.string),\n",
      "        }),\n",
      "        'context': Text(shape=(), dtype=tf.string),\n",
      "        'id': tf.string,\n",
      "        'question': Text(shape=(), dtype=tf.string),\n",
      "        'title': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=98169,\n",
      "    splits={\n",
      "        'train': 87599,\n",
      "        'validation': 10570,\n",
      "    },\n",
      "    supervised_keys=None,\n",
      "    citation=\"\"\"@article{2016arXiv160605250R,\n",
      "           author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
      "                     Konstantin and {Liang}, Percy},\n",
      "            title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
      "          journal = {arXiv e-prints},\n",
      "             year = 2016,\n",
      "              eid = {arXiv:1606.05250},\n",
      "            pages = {arXiv:1606.05250},\n",
      "    archivePrefix = {arXiv},\n",
      "           eprint = {1606.05250},\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, info = tfds.load('squad', split='train', with_info=True)\n",
    "valid_dataset = tfds.load('squad', split='validation', with_info=False)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7EK-tgHDXOM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data from the dataset: \n",
      " {'answers': {'answer_start': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([427], dtype=int32)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mobile phones'], dtype=object)>}, 'context': <tf.Tensor: shape=(), dtype=string, numpy=b'The difference in the above factors for the case of \\xce\\xb8=0 is the reason that most broadcasting (transmissions intended for the public) uses vertical polarization. For receivers near the ground, horizontally polarized transmissions suffer cancellation. For best reception the receiving antennas for these signals are likewise vertically polarized. In some applications where the receiving antenna must work in any position, as in mobile phones, the base station antennas use mixed polarization, such as linear polarization at an angle (with both vertical and horizontal components) or circular polarization.'>, 'id': <tf.Tensor: shape=(), dtype=string, numpy=b'57306bf68ab72b1400f9c4dc'>, 'question': <tf.Tensor: shape=(), dtype=string, numpy=b'What is one use that would require an antenna to receive signals in various ways at once?'>, 'title': <tf.Tensor: shape=(), dtype=string, numpy=b'Antenna_(radio)'>}\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_dataset))\n",
    "print(\"Example data from the dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIZFbP4IDXOO",
    "outputId": "7e5e1ad2-fdd7-448b-c32a-71208ddfa158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps:  21900\n",
      "Total Validation Steps:  2643\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = 1e4\n",
    "batch_size = 4\n",
    "max_len = 100\n",
    "buffer_size = 1000\n",
    "ntrain = info.splits[\"train\"].num_examples\n",
    "nvalid = info.splits[\"validation\"].num_examples\n",
    "steps = int(np.ceil(ntrain/batch_size))\n",
    "valid_steps = int(np.ceil(nvalid/batch_size))\n",
    "print(\"Total Steps: \", steps)\n",
    "print(\"Total Validation Steps: \", valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdhcP171DXOY"
   },
   "outputs": [],
   "source": [
    "def encode(context,question ,answer, max_length=max_len):\n",
    "    question_plus = f\"answer_me: {str(question.numpy().decode('utf-8'))}\"\n",
    "    question_plus += f\" context: {str(context.numpy().decode('utf-8'))}  </s>\"\n",
    "    \n",
    "    answer_plus = ', '.join([i.decode('utf-8') for i in list(answer.numpy())])\n",
    "    answer_plus = f\"{answer_plus} </s>\"\n",
    "    \n",
    "    encoder_inputs = tokenizer(question_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=max_length,\n",
    "                              pad_to_max_length=True)\n",
    "    \n",
    "    decoder_inputs = tokenizer(answer_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=max_length,\n",
    "                              pad_to_max_length=True)\n",
    "    \n",
    "    input_ids = encoder_inputs['input_ids'][0]\n",
    "    input_attention = encoder_inputs['attention_mask'][0]\n",
    "    target_ids = decoder_inputs['input_ids'][0]\n",
    "    target_attention = decoder_inputs['attention_mask'][0]\n",
    "    \n",
    "    return input_ids,input_attention, target_ids, target_attention\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sdlkpyEDXOa"
   },
   "outputs": [],
   "source": [
    "def encode_tf(inputs):\n",
    "    context = inputs['context']\n",
    "    question = inputs['question']\n",
    "    answer = inputs['answers']['text']\n",
    "    encoded = tf.py_function(encode, [context, question, answer], \n",
    "                                           [tf.int32, tf.int32, tf.int32, tf.int32])\n",
    "    input_ids,input_attention, target_ids,target_attention = encoded\n",
    "    input_ids.set_shape([None])\n",
    "    target_ids.set_shape([None])\n",
    "    input_attention.set_shape([None])\n",
    "    target_attention.set_shape([None])\n",
    "    \n",
    "    labels = tf.reshape(target_ids, [-1, 1])\n",
    "    data=  {'input_ids': input_ids, #'decoder_input_ids': target_ids, \n",
    "            'labels': target_ids, \n",
    "            'attention_mask': input_attention,\n",
    "           'decoder_attention_mask': target_attention}\n",
    "    return (data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUKypEaSDXOd"
   },
   "outputs": [],
   "source": [
    "def create_dataset(source_dataset, cache_path=None, batch_size=4, \n",
    "                   buffer_size= 1000, shuffling=True):\n",
    "    dataset = source_dataset.map(encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if cache_path is not None:\n",
    "        dataset = dataset.cache(cache_path)        \n",
    "    if shuffling:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNiNtnsHDXOf"
   },
   "outputs": [],
   "source": [
    "train_ds= create_dataset(train_dataset, batch_size=batch_size, \n",
    "                         shuffling=True, cache_path = None)\n",
    "valid_ds = create_dataset(valid_dataset, batch_size=batch_size, \n",
    "                         shuffling=False, cache_path = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KL9VfwI6DXOh",
    "outputId": "09cd331a-338c-43c7-d0cd-1a22e685232a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1768: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': <tf.Tensor: shape=(4, 100), dtype=int32, numpy=\n",
       "  array([[ 1525,   834,   526,    10,   571,   231,    47,     8,     3,\n",
       "          15829,  6878,    53,  1154,    28,     8,  6552,  3815,  1494,\n",
       "             16,  1230,    58,  2625,    10,    37,  6552,  3815,    19,\n",
       "           6878,    16,     8,   907,  1323,   190,     3, 15829,  5716,\n",
       "              5,  6552,  3815, 17831,  2009,    65,  1936,  7313,     6,\n",
       "             28,     3, 15829,    11,     3, 15829,  8544,     3,     9,\n",
       "          23980,     3,     9,  1368, 10635, 23938, 13569,    16,     8,\n",
       "           1412,   104,  1808,   774,     6,    95,   850,  5953,    45,\n",
       "           1673,   104,  2368,   116,  2591,   341,     3,  2378,    26,\n",
       "             30,  7547, 23375,    11, 25043,    87, 26130,   567,   357,\n",
       "           4743, 13922, 13569,   201,    11,     3, 15829,  5716,    65,\n",
       "            118],\n",
       "         [ 1525,   834,   526,    10,  4073,  3262,  8996,    16, 26660,\n",
       "             19,   801,    12,    36,     3,     9,   234,    21,     3,\n",
       "          10673,    52,  6546,  6331,    58,  2625,    10, 26660,    31,\n",
       "              7, 17721,    11,     8,     3,     7,  8745,    53,  7572,\n",
       "             13,   165,   731,    18,   120,    53, 11787,   370, 11341,\n",
       "             21,    46, 19075,    13,     3,    89,   322,     9,    11,\n",
       "          29908,     5,    37,  5827,  1719,    16,    11,   300,     8,\n",
       "            690, 18253,    15,     7,   844,    13, 19517,    11, 11432,\n",
       "           3172,     6,    84,    33, 18570,    16,     8,   607,    13,\n",
       "           1157,  9307,     6,     3,   172,    32,    32,     7,     6,\n",
       "           3016,    18,   172,    32,    32,     7,    11,     3,     9,\n",
       "           9106],\n",
       "         [ 1525,   834,   526,    10,   125,    47, 11016,  5811,     7,\n",
       "             40,   232,    31,     7,  1075,    16,   789,    58,  2625,\n",
       "             10,    37,  2015,  5919,    13,  3452,  2061,    16, 19201,\n",
       "            741,    15,    26,    45,     3,     9,  1291,  1357,  1026,\n",
       "             16, 19201,    57, 11016,  5811,     7,    40,   232,     6,\n",
       "           7471,    13,  1015,    21,  2855,    16,     8, 18969,   104,\n",
       "           2294,  2518, 16117,   789,     5,    37,  1291,  1357,    47,\n",
       "           6960,    57, 13054,  4866,   335,    87,  4122,     6,    46,\n",
       "           8033,    12,   415,  1073,  5779,    12,   515,    21,  6113,\n",
       "              5,  4375,     3,     7,   144,     8,   850,  1220,  6498,\n",
       "             16,    70,   336,   215,    13,  2329,  1073,    11,   130,\n",
       "           1622],\n",
       "         [ 1525,   834,   526,    10,   363,   686,    13,   723,   405,\n",
       "            925, 17363,    32,  1912,    58,  2625,    10,    37, 18715,\n",
       "             29, 21849, 10812,    15,  2953, 14047,     3,    15, 10673,\n",
       "           2661,    24,  2237,    12,     8,  7025,    13, 18715,    29,\n",
       "              7,    16,   796,  1440,    16,     8,   296,     5, 18715,\n",
       "             29,     7,  2697,    12,    70, 10822,    11,   824,  1227,\n",
       "              9, 11842,  3247,  4659,    12, 10393,    28,    70,   723,\n",
       "              5,    86,     8,   442,    18, 13714,    32, 10812,    15,\n",
       "          18715,    29,   573,    13,     8,   907,  1323,     6,     8,\n",
       "             78,    18,  9341,    96,  1050,    89,   121,   869, 18715,\n",
       "             29,  2595,   723,     6,   338, 18715,    29,    11,  4551,\n",
       "           7345]], dtype=int32)>,\n",
       "  'labels': <tf.Tensor: shape=(4, 100), dtype=int32, numpy=\n",
       "  array([[ 1970,  2108,     1,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [ 5192,   152,  1703,    76,  2154,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [ 7471,    13,  1015,    21,  2855,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [ 2783,   723,  3334,    28, 18715,    29, 12702,   723,     1,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]], dtype=int32)>,\n",
       "  'attention_mask': <tf.Tensor: shape=(4, 100), dtype=int32, numpy=\n",
       "  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>,\n",
       "  'decoder_attention_mask': <tf.Tensor: shape=(4, 100), dtype=int32, numpy=\n",
       "  array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>},\n",
       " <tf.Tensor: shape=(4, 100, 1), dtype=int32, numpy=\n",
       " array([[[ 1970],\n",
       "         [ 2108],\n",
       "         [    1],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]],\n",
       " \n",
       "        [[ 5192],\n",
       "         [  152],\n",
       "         [ 1703],\n",
       "         [   76],\n",
       "         [ 2154],\n",
       "         [    1],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]],\n",
       " \n",
       "        [[ 7471],\n",
       "         [   13],\n",
       "         [ 1015],\n",
       "         [   21],\n",
       "         [ 2855],\n",
       "         [    1],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]],\n",
       " \n",
       "        [[ 2783],\n",
       "         [  723],\n",
       "         [ 3334],\n",
       "         [   28],\n",
       "         [18715],\n",
       "         [   29],\n",
       "         [12702],\n",
       "         [  723],\n",
       "         [    1],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]]], dtype=int32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_ds))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHq71CTDDXOn"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, warmup_steps=1e4, schedule=True, initial_lr = 1e-3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    m = tf.maximum(self.warmup_steps, step)\n",
    "    m = tf.cast(m, tf.float32)\n",
    "    lr = tf.math.rsqrt(m)\n",
    "    \n",
    "    return lr \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6-LcG6TDXOt",
    "outputId": "26461795-21ed-418e-d843-415152d291ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Learning rate')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "schedule = CustomSchedule()\n",
    "plt.plot(schedule(tf.range(25000, dtype=tf.float32)))\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rA9OVe28DXOq"
   },
   "outputs": [],
   "source": [
    "start_profile_batch = steps+10\n",
    "stop_profile_batch = start_profile_batch + 100\n",
    "profile_range = f\"{start_profile_batch},{stop_profile_batch}\"\n",
    "\n",
    "log_path = log_dir + \"/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_path, histogram_freq=1,\n",
    "                                                     update_freq=20,profile_batch=profile_range)\n",
    "\n",
    "checkpoint_filepath = save_path + \"/\" + \"T5-{epoch:04d}-{val_loss:.4f}.ckpt\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "callbacks = [tensorboard_callback, model_checkpoint_callback] \n",
    "metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(name='accuracy') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SX_CQGhoDXOw"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule()\n",
    "# learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2vcCZyyDXOy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZym7SmWDXO0",
    "outputId": "a2d7251a-59e9-4058-ef95-cde31265c8e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_tf_utils:All model checkpoint weights were used when initializing SnapthatT5.\n",
      "\n",
      "WARNING:transformers.modeling_tf_utils:Some weights of SnapthatT5 were not initialized from the model checkpoint at t5-base and are newly initialized: ['loss']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SnapthatT5.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_QxL8gPDXO1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8QOVpFxDXO5",
    "outputId": "9e187954-459a-4770-f383-a1501b21e90f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1768: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21900/21900 [==============================] - 9771s 446ms/step - accuracy: 0.9571 - loss: 0.4131 - lr: 0.0089 - val_accuracy: 0.8610 - val_loss: 1.3091\n",
      "Epoch 2/50\n",
      "  109/21900 [..............................] - ETA: 2:36:47 - accuracy: 0.9571 - loss: 0.4001 - lr: 0.0067WARNING:tensorflow:From /home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8291/21900 [==========>...................] - ETA: 1:38:46 - accuracy: 0.9575 - loss: 0.4015 - lr: 0.0062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1e56b2793c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(train_ds, epochs=50, steps_per_epoch=steps, callbacks=callbacks, \n\u001b[0;32m----> 3\u001b[0;31m           validation_data=valid_ds, validation_steps=valid_steps, initial_epoch=epochs_done)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_done = 0\n",
    "model.fit(train_ds, epochs=50, steps_per_epoch=steps, callbacks=callbacks, \n",
    "          validation_data=valid_ds, validation_steps=valid_steps, initial_epoch=epochs_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDIaNYrVDXO7"
   },
   "outputs": [],
   "source": [
    "# model.evaluate(valid_ds, steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYDPH77LDXO9"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets tests our model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We went on a trip to Europe. We had our breakfast at 7 am in the morning at the nearby coffee shop. Wore a dark blue over coat for our first visit to Louvre Museum to experience history and art\n",
      "At what time did we had breakfast?\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"We went on a trip to Europe. We had our breakfast at 7 am in the morning at \\\n",
    "the nearby coffee shop. Wore a dark blue over coat for our first visit to Louvre Museum \\\n",
    "to experience history and art.\"\"\"\n",
    "\n",
    "question = \"At what time did we had breakfast?\"\n",
    "print(context)\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ihk1kxxDXO_"
   },
   "outputs": [],
   "source": [
    "input_text =  f\"answer_me: {question} context: {context} </s>\"\n",
    "encoded_query = tokenizer(input_text, \n",
    "                         return_tensors='tf', padding=True, truncation=True)\n",
    "input_ids = encoded_query[\"input_ids\"]\n",
    "attention_mask = encoded_query[\"attention_mask\"]\n",
    "generated_question = model.generate(input_ids, attention_mask=attention_mask)\n",
    "decoded_question = tokenizer.decode(generated_question.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EejrdGS6DXPB",
    "outputId": "f2747344-4e8d-4794-8d92-00f4a6f87616"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRuos-M-DXPM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "TF-T5- Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Hugging Face",
   "language": "python",
   "name": "hugging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
